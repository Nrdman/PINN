{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e076795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE import lhs\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3ad3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates exact data\n",
    "\n",
    "data  = np.zeros([101, 101]) \n",
    "x = np.zeros([101])\n",
    "y = np.zeros([101])\n",
    "pi = math.pi\n",
    "const = -0.5*pi\n",
    "for i in range(101):\n",
    "    x[i] = (i)/100\n",
    "    for j in range(101):\n",
    "        if i==0:\n",
    "            y[j]=j/100\n",
    "        data[i,j] = math.sin(x[i]*const)*(math.e)**(y[j]*const)\n",
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5156c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 1)\n",
      "[[-0.        ]\n",
      " [-0.01570732]\n",
      " [-0.03141076]\n",
      " [-0.04710645]\n",
      " [-0.06279052]\n",
      " [-0.0784591 ]\n",
      " [-0.09410831]\n",
      " [-0.10973431]\n",
      " [-0.12533323]\n",
      " [-0.14090123]\n",
      " [-0.15643447]\n",
      " [-0.1719291 ]\n",
      " [-0.18738131]\n",
      " [-0.2027873 ]\n",
      " [-0.21814324]\n",
      " [-0.23344536]\n",
      " [-0.24868989]\n",
      " [-0.26387305]\n",
      " [-0.27899111]\n",
      " [-0.29404033]\n",
      " [-0.30901699]\n",
      " [-0.32391742]\n",
      " [-0.33873792]\n",
      " [-0.35347484]\n",
      " [-0.36812455]\n",
      " [-0.38268343]\n",
      " [-0.39714789]\n",
      " [-0.41151436]\n",
      " [-0.42577929]\n",
      " [-0.43993917]\n",
      " [-0.4539905 ]\n",
      " [-0.46792981]\n",
      " [-0.48175367]\n",
      " [-0.49545867]\n",
      " [-0.50904142]\n",
      " [-0.52249856]\n",
      " [-0.53582679]\n",
      " [-0.54902282]\n",
      " [-0.56208338]\n",
      " [-0.57500525]\n",
      " [-0.58778525]\n",
      " [-0.60042023]\n",
      " [-0.61290705]\n",
      " [-0.62524266]\n",
      " [-0.63742399]\n",
      " [-0.64944805]\n",
      " [-0.66131187]\n",
      " [-0.67301251]\n",
      " [-0.68454711]\n",
      " [-0.6959128 ]\n",
      " [-0.70710678]\n",
      " [-0.7181263 ]\n",
      " [-0.72896863]\n",
      " [-0.73963109]\n",
      " [-0.75011107]\n",
      " [-0.76040597]\n",
      " [-0.77051324]\n",
      " [-0.78043041]\n",
      " [-0.79015501]\n",
      " [-0.79968466]\n",
      " [-0.80901699]\n",
      " [-0.81814972]\n",
      " [-0.82708057]\n",
      " [-0.83580736]\n",
      " [-0.84432793]\n",
      " [-0.85264016]\n",
      " [-0.86074203]\n",
      " [-0.86863151]\n",
      " [-0.87630668]\n",
      " [-0.88376563]\n",
      " [-0.89100652]\n",
      " [-0.89802758]\n",
      " [-0.90482705]\n",
      " [-0.91140328]\n",
      " [-0.91775463]\n",
      " [-0.92387953]\n",
      " [-0.92977649]\n",
      " [-0.93544403]\n",
      " [-0.94088077]\n",
      " [-0.94608536]\n",
      " [-0.95105652]\n",
      " [-0.95579301]\n",
      " [-0.96029369]\n",
      " [-0.96455742]\n",
      " [-0.96858316]\n",
      " [-0.97236992]\n",
      " [-0.97591676]\n",
      " [-0.97922281]\n",
      " [-0.98228725]\n",
      " [-0.98510933]\n",
      " [-0.98768834]\n",
      " [-0.99002366]\n",
      " [-0.9921147 ]\n",
      " [-0.99396096]\n",
      " [-0.99556196]\n",
      " [-0.99691733]\n",
      " [-0.99802673]\n",
      " [-0.99888987]\n",
      " [-0.99950656]\n",
      " [-0.99987663]\n",
      " [-1.        ]\n",
      " [-0.        ]\n",
      " [-0.00326523]\n",
      " [-0.00652966]\n",
      " [-0.00979247]\n",
      " [-0.01305287]\n",
      " [-0.01631004]\n",
      " [-0.0195632 ]\n",
      " [-0.02281152]\n",
      " [-0.02605422]\n",
      " [-0.02929049]\n",
      " [-0.03251953]\n",
      " [-0.03574055]\n",
      " [-0.03895275]\n",
      " [-0.04215534]\n",
      " [-0.04534752]\n",
      " [-0.04852852]\n",
      " [-0.05169755]\n",
      " [-0.05485382]\n",
      " [-0.05799655]\n",
      " [-0.06112498]\n",
      " [-0.06423832]\n",
      " [-0.06733582]\n",
      " [-0.0704167 ]\n",
      " [-0.0734802 ]\n",
      " [-0.07652558]\n",
      " [-0.07955207]\n",
      " [-0.08255894]\n",
      " [-0.08554543]\n",
      " [-0.08851082]\n",
      " [-0.09145437]\n",
      " [-0.09437535]\n",
      " [-0.09727305]\n",
      " [-0.10014675]\n",
      " [-0.10299574]\n",
      " [-0.10581931]\n",
      " [-0.10861678]\n",
      " [-0.11138745]\n",
      " [-0.11413063]\n",
      " [-0.11684565]\n",
      " [-0.11953185]\n",
      " [-0.12218855]\n",
      " [-0.1248151 ]\n",
      " [-0.12741086]\n",
      " [-0.12997518]\n",
      " [-0.13250743]\n",
      " [-0.13500699]\n",
      " [-0.13747323]\n",
      " [-0.13990556]\n",
      " [-0.14230336]\n",
      " [-0.14466606]\n",
      " [-0.14699306]\n",
      " [-0.14928379]\n",
      " [-0.15153769]\n",
      " [-0.1537542 ]\n",
      " [-0.15593277]\n",
      " [-0.15807287]\n",
      " [-0.16017397]\n",
      " [-0.16223554]\n",
      " [-0.16425709]\n",
      " [-0.16623811]\n",
      " [-0.16817811]\n",
      " [-0.17007662]\n",
      " [-0.17193316]\n",
      " [-0.17374728]\n",
      " [-0.17551853]\n",
      " [-0.17724648]\n",
      " [-0.17893069]\n",
      " [-0.18057075]\n",
      " [-0.18216626]\n",
      " [-0.18371682]\n",
      " [-0.18522206]\n",
      " [-0.18668159]\n",
      " [-0.18809506]\n",
      " [-0.18946213]\n",
      " [-0.19078244]\n",
      " [-0.19205569]\n",
      " [-0.19328154]\n",
      " [-0.19445971]\n",
      " [-0.1955899 ]\n",
      " [-0.19667182]\n",
      " [-0.19770523]\n",
      " [-0.19868985]\n",
      " [-0.19962544]\n",
      " [-0.20051179]\n",
      " [-0.20134866]\n",
      " [-0.20213585]\n",
      " [-0.20287316]\n",
      " [-0.20356042]\n",
      " [-0.20419746]\n",
      " [-0.20478411]\n",
      " [-0.20532023]\n",
      " [-0.2058057 ]\n",
      " [-0.20624038]\n",
      " [-0.20662418]\n",
      " [-0.206957  ]\n",
      " [-0.20723875]\n",
      " [-0.20746937]\n",
      " [-0.2076488 ]\n",
      " [-0.207777  ]\n",
      " [-0.20785393]\n",
      " [-0.20787958]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [-1.        ]\n",
      " [-0.98441476]\n",
      " [-0.96907243]\n",
      " [-0.9539692 ]\n",
      " [-0.93910137]\n",
      " [-0.92446525]\n",
      " [-0.91005724]\n",
      " [-0.89587378]\n",
      " [-0.88191138]\n",
      " [-0.86816658]\n",
      " [-0.854636  ]\n",
      " [-0.84131629]\n",
      " [-0.82820418]\n",
      " [-0.81529642]\n",
      " [-0.80258984]\n",
      " [-0.79008128]\n",
      " [-0.77776768]\n",
      " [-0.76564599]\n",
      " [-0.75371321]\n",
      " [-0.74196641]\n",
      " [-0.73040269]\n",
      " [-0.71901919]\n",
      " [-0.70781311]\n",
      " [-0.69678167]\n",
      " [-0.68592217]\n",
      " [-0.67523191]\n",
      " [-0.66470826]\n",
      " [-0.65434862]\n",
      " [-0.64415044]\n",
      " [-0.63411121]\n",
      " [-0.62422843]\n",
      " [-0.61449969]\n",
      " [-0.60492256]\n",
      " [-0.5954947 ]\n",
      " [-0.58621378]\n",
      " [-0.5770775 ]\n",
      " [-0.56808361]\n",
      " [-0.55922989]\n",
      " [-0.55051416]\n",
      " [-0.54193426]\n",
      " [-0.53348809]\n",
      " [-0.52517355]\n",
      " [-0.5169886 ]\n",
      " [-0.50893121]\n",
      " [-0.5009994 ]\n",
      " [-0.4931912 ]\n",
      " [-0.4855047 ]\n",
      " [-0.47793799]\n",
      " [-0.47048922]\n",
      " [-0.46315653]\n",
      " [-0.45593813]\n",
      " [-0.44883222]\n",
      " [-0.44183707]\n",
      " [-0.43495093]\n",
      " [-0.42817212]\n",
      " [-0.42149896]\n",
      " [-0.41492979]\n",
      " [-0.40846302]\n",
      " [-0.40209702]\n",
      " [-0.39583025]\n",
      " [-0.38966114]\n",
      " [-0.38358818]\n",
      " [-0.37760986]\n",
      " [-0.37172472]\n",
      " [-0.36593131]\n",
      " [-0.36022818]\n",
      " [-0.35461394]\n",
      " [-0.3490872 ]\n",
      " [-0.34364659]\n",
      " [-0.33829078]\n",
      " [-0.33301844]\n",
      " [-0.32782826]\n",
      " [-0.32271898]\n",
      " [-0.31768933]\n",
      " [-0.31273807]\n",
      " [-0.30786397]\n",
      " [-0.30306584]\n",
      " [-0.29834249]\n",
      " [-0.29369275]\n",
      " [-0.28911548]\n",
      " [-0.28460954]\n",
      " [-0.28017384]\n",
      " [-0.27580726]\n",
      " [-0.27150874]\n",
      " [-0.26727721]\n",
      " [-0.26311163]\n",
      " [-0.25901098]\n",
      " [-0.25497423]\n",
      " [-0.25100039]\n",
      " [-0.24708849]\n",
      " [-0.24323756]\n",
      " [-0.23944665]\n",
      " [-0.23571481]\n",
      " [-0.23204114]\n",
      " [-0.22842473]\n",
      " [-0.22486467]\n",
      " [-0.2213601 ]\n",
      " [-0.21791015]\n",
      " [-0.21451397]\n",
      " [-0.21117072]\n",
      " [-0.20787958]]\n"
     ]
    }
   ],
   "source": [
    "#makes a list of every x y combination and a list of the values\n",
    "X, Y = np.meshgrid(x,y) \n",
    "X_star=np.hstack((X.flatten()[:,None],Y.flatten()[:,None])) \n",
    "u_star = data.flatten()[:,None] \n",
    "\n",
    "xx1=np.hstack((X[0:1,:].T,Y[0:1,:].T))  # coords for initial boundary condition  y=0\n",
    "xx2 = np.hstack((X[-1:,:].T,Y[-1:,:].T)) #coords for initial bc y=1\n",
    "xx3=np.hstack((X[:,0:1],Y[:,0:1]))      # coords for boundary condition for x=0 \n",
    "xx4=np.hstack((X[:,-1:],Y[:,-1:]))      # coords for boundary condition for x=1\n",
    "        \n",
    "uu1 = data[0:1,:].T #data for y=0, this transpose is also present in the other code\n",
    "uu2 = data[-1:, :].T #data for y =1\n",
    "uu3 = data[:,0:1] #data for x =0\n",
    "uu4 = data[:,-1:] #data for x=1\n",
    "actual_outputs_1  = data[25:26,:].T\n",
    "actual_outputs_2  = data[50:51,:].T\n",
    "actual_outputs_3  = data[75:76,:].T\n",
    "\n",
    "lb = X_star.min(axis=0)    # (1,2)   [-1 0]\n",
    "ub = X_star.max(axis=0)    # (1,2)   [1 0.99]\n",
    "no_of_interior_points=2500 #8000\n",
    "no_of_collocation_points=200 #100\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3, xx4])                           #  stacking up all data related to boundary conditions \n",
    "X_f_train = lb + (ub-lb)*lhs(2, no_of_interior_points)           # lhs is used to generate random sample of points. 2 is no of variables(x,t).\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))    \n",
    "u_train = np.vstack([uu1, uu2, uu3, uu4])                             #  values correspoing to X_u_train\n",
    "print(u_train.shape)\n",
    "print(u_train)\n",
    "idx = np.random.choice(X_u_train.shape[0], no_of_collocation_points , replace=False) # random sample of collocation points from boundary coords\n",
    "X_u_train = X_u_train[idx, :]     # Those collocation points chosen from boundary conditions data\n",
    "u_train = u_train[idx,:]     # Output corresponding to collocation points\n",
    "\n",
    "\n",
    "# subscript u denotes collocation/boundary points and subscript f denotes interior points\n",
    "x_u = X_u_train[:,0:1]       #Separating x,t from X_u_train\n",
    "y_u = X_u_train[:,1:2]\n",
    "x_f = X_f_train [:,0:1]\n",
    "y_f = X_f_train [:,1:2]\n",
    "\n",
    "x_u_tf=tf.Variable(x_u)       #Converting to tensor variable. Essential for calculating gradients later on.\n",
    "y_u_tf=tf.Variable(y_u)\n",
    "x_f_tf=tf.Variable(x_f)\n",
    "y_f_tf=tf.Variable(y_f)\n",
    "X_f_train_tf=tf.Variable(X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6df88a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "def get_model():\n",
    "    model=Sequential([Dense(20,activation='tanh',\n",
    "                            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                            input_shape=(2,),name='H1')])\n",
    "    for i in range(6):\n",
    "        model.add(Dense(20,activation='tanh',kernel_initializer=tf.keras.initializers.GlorotNormal(),name='H'+str(i+2)))\n",
    "    model.add(Dense(1,name='output_layer'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b393ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " H1 (Dense)                  (None, 20)                60        \n",
      "                                                                 \n",
      " H2 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " H3 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " H4 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " H5 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " H6 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " H7 (Dense)                  (None, 20)                420       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,601\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e68631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating loss wrt to interior points\n",
    "def interior_loss():\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_f_train_tf)\n",
    "        with tf.GradientTape() as tape2:\n",
    "            u_predicted=model(X_f_train_tf)\n",
    "        grad=tape2.gradient(u_predicted,X_f_train_tf)\n",
    "        du_dx=grad[:, 0]\n",
    "        du_dy=grad[:, 1]\n",
    "    j = tape.gradient(grad, X_f_train_tf)\n",
    "    d2u_dx2 = j[:,0]\n",
    "    d2u_dy2 = j[:,1]\n",
    "    #u_predicted=tf.cast(u_predicted, dtype=tf.float64)\n",
    "    d2u_dx2=tf.reshape(d2u_dx2, [len(d2u_dx2),1])\n",
    "    d2u_dy2=tf.reshape(d2u_dy2, [len(d2u_dy2),1])\n",
    "    #d2u_dx2=tf.cast(d2u_dx2, dtype=tf.float64)\n",
    "    #d2u_dy2=tf.cast(d2u_dy2, dtype=tf.float64)\n",
    "    f= -d2u_dx2-d2u_dy2  #want to f to minimize to 0\n",
    "    f=tf.math.reduce_mean(tf.math.square(f)) #squares elementwise and finds average \n",
    "    #f = tf.cast(f, dtype = tf.float64)\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a89cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LBFG-S, which is second order optimizer, has been used to update the weights and biases because conventional first order optimizers Adam, Gradient descent and RMSprop \n",
    "are slow to converge.LBFG-S is not available by default in Tensorflow 2.0 and hence a function from Tensorflow Probability has been used. This has not been coded by me\n",
    "except for a minor addition to loss function(check the loss_value variable). Please refer to the link below to get a better idea.\n",
    "\n",
    "https://gist.github.com/piyueh/712ec7d4540489aad2dcfb80f9a54993\n",
    "\n",
    "\"\"\"\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def function_factory(model, loss, train_x, train_y):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a function with signature loss_value = loss(pred_y, true_y).\n",
    "        train_x [in]: the input part of training data.\n",
    "        train_y [in]: the output part of training data.\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = numpy.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "    part = tf.constant(part)\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # now create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "\n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            # update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            # calculate the loss\n",
    "            loss_value = loss(model(train_x, training=True), train_y)\n",
    "            int_loss=interior_loss()\n",
    "            loss_value=loss_value+int_loss\n",
    "\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "        \n",
    "        loss_value = tf.cast(loss_value, tf.float32)\n",
    "        #grads = tf.cast(grads, tf.float64)\n",
    "        return loss_value, grads\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c5e0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_helper(inputs, outputs, title, fname):\n",
    "\n",
    "    pyplot.figure(figsize=(8,4))\n",
    "    pyplot.tricontourf(inputs[:, 1], inputs[:, 0], outputs.flatten(), 100)\n",
    "    pyplot.scatter(X_u_train[:, 1], X_u_train[:, 0],marker='x',s=100,c='k')\n",
    "    pyplot.xlabel(\"y\")\n",
    "    pyplot.ylabel(\"x\")\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()\n",
    "    pyplot.savefig(fname)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72e5bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 loss: 0.88625887401488579\n",
      "Iter: 2 loss: 3.3211531671679455\n",
      "Iter: 3 loss: 2.7176429326196558\n",
      "Iter: 4 loss: 2.2773941545037832\n",
      "Iter: 5 loss: 1.3036204895077979\n",
      "Iter: 6 loss: 1.2900757393158968\n",
      "Iter: 7 loss: 1.2026707300184658\n",
      "Iter: 8 loss: 2.5875463673515222\n",
      "Iter: 9 loss: 3.1422669289116989\n",
      "Iter: 10 loss: 2.5175570650675514\n",
      "Iter: 11 loss: 0.596452549119131\n",
      "Iter: 12 loss: 0.19968568285840196\n",
      "Iter: 13 loss: 0.093955056631320907\n",
      "Iter: 14 loss: 0.53769669554718191\n",
      "Iter: 15 loss: 0.084272064262048349\n",
      "Iter: 16 loss: 0.0704446579077644\n",
      "Iter: 17 loss: 0.0676394320311869\n",
      "Iter: 18 loss: 0.058716232452932178\n",
      "Iter: 19 loss: 0.045273645046311248\n",
      "Iter: 20 loss: 0.060838483483262949\n",
      "Iter: 21 loss: 0.037829996869754551\n",
      "Iter: 22 loss: 0.03437911140079862\n",
      "Iter: 23 loss: 0.042785204511354653\n",
      "Iter: 24 loss: 0.033116856408280278\n",
      "Iter: 25 loss: 0.031875900398964048\n",
      "Iter: 26 loss: 0.028026687362591486\n",
      "Iter: 27 loss: 0.03337762841335884\n",
      "Iter: 28 loss: 0.025368898790949418\n",
      "Iter: 29 loss: 0.020303495815082905\n",
      "Iter: 30 loss: 0.027718754715156342\n",
      "Iter: 31 loss: 0.017871152810541222\n",
      "Iter: 32 loss: 0.015824502224522689\n",
      "Iter: 33 loss: 0.021130037195426561\n",
      "Iter: 34 loss: 0.014983289603910092\n",
      "Iter: 35 loss: 0.014057119969553177\n",
      "Iter: 36 loss: 0.023441975816033842\n",
      "Iter: 37 loss: 0.013928441717527917\n",
      "Iter: 38 loss: 0.0133178109098607\n",
      "Iter: 39 loss: 0.012739274795470499\n",
      "Iter: 40 loss: 0.012698886098881347\n",
      "Iter: 41 loss: 0.012133606680409019\n",
      "Iter: 42 loss: 0.011948228595035828\n",
      "Iter: 43 loss: 0.011088724955087474\n",
      "Iter: 44 loss: 0.010558210775310748\n",
      "Iter: 45 loss: 0.00991131540666994\n",
      "Iter: 46 loss: 0.013423982228719575\n",
      "Iter: 47 loss: 0.0098270388629113324\n",
      "Iter: 48 loss: 0.00940268032356287\n",
      "Iter: 49 loss: 0.01625636655467777\n",
      "Iter: 50 loss: 0.0093929587423711575\n",
      "Iter: 51 loss: 0.0092834505264654064\n",
      "Iter: 52 loss: 0.0092268400377257687\n",
      "Iter: 53 loss: 0.00907835731813746\n",
      "Iter: 54 loss: 0.009753552367384552\n",
      "Iter: 55 loss: 0.009040725617593566\n",
      "Iter: 56 loss: 0.0088793716686327382\n",
      "Iter: 57 loss: 0.0087580439139693014\n",
      "Iter: 58 loss: 0.008718580695292159\n",
      "Iter: 59 loss: 0.0084313172212043316\n",
      "Iter: 60 loss: 0.0083536082426969045\n",
      "Iter: 61 loss: 0.0081865434900813046\n",
      "Iter: 62 loss: 0.0079688752812264\n",
      "Iter: 63 loss: 0.010485070794526694\n",
      "Iter: 64 loss: 0.0079637247580204209\n",
      "Iter: 65 loss: 0.00780182711913352\n",
      "Iter: 66 loss: 0.0078017974888844113\n",
      "Iter: 67 loss: 0.0077024362499032162\n",
      "Iter: 68 loss: 0.007627083094273724\n",
      "Iter: 69 loss: 0.0076002515230028239\n",
      "Iter: 70 loss: 0.0073634566384889587\n",
      "Iter: 71 loss: 0.0073909835304387037\n",
      "Iter: 72 loss: 0.0071957838049190219\n",
      "Iter: 73 loss: 0.0070265443687249612\n",
      "Iter: 74 loss: 0.0072063554474437854\n",
      "Iter: 75 loss: 0.0069386160390299545\n",
      "Iter: 76 loss: 0.0067631968981856222\n",
      "Iter: 77 loss: 0.0099503079960032367\n",
      "Iter: 78 loss: 0.006760604314297162\n",
      "Iter: 79 loss: 0.0066555843847842791\n",
      "Iter: 80 loss: 0.0069528214468736072\n",
      "Iter: 81 loss: 0.0066163091025466577\n",
      "Iter: 82 loss: 0.0065028821767689465\n",
      "Iter: 83 loss: 0.010891977792248772\n",
      "Iter: 84 loss: 0.0065026993810142156\n",
      "Iter: 85 loss: 0.0063673613234628132\n",
      "Iter: 86 loss: 0.0082060543557426514\n",
      "Iter: 87 loss: 0.0063664625490226917\n",
      "Iter: 88 loss: 0.0062466948918595833\n",
      "Iter: 89 loss: 0.0074703570839576346\n",
      "Iter: 90 loss: 0.0062435590679156081\n",
      "Iter: 91 loss: 0.0060650739738541491\n",
      "Iter: 92 loss: 0.0077880472021684931\n",
      "Iter: 93 loss: 0.0060421974218024524\n",
      "Iter: 94 loss: 0.0057442436292541015\n",
      "Iter: 95 loss: 0.0068501952163989525\n",
      "Iter: 96 loss: 0.0056350736863518642\n",
      "Iter: 97 loss: 0.0061750458054661292\n",
      "Iter: 98 loss: 0.0054737242483517351\n",
      "Iter: 99 loss: 0.0054759202163685633\n",
      "Iter: 100 loss: 0.00493988967009864\n",
      "Iter: 101 loss: 0.0047500792212439545\n",
      "Iter: 102 loss: 0.00705073770790095\n",
      "Iter: 103 loss: 0.0047496377143596245\n",
      "Iter: 104 loss: 0.004613883721687243\n",
      "Iter: 105 loss: 0.0053981694216033473\n",
      "Iter: 106 loss: 0.0045951577987665033\n",
      "Iter: 107 loss: 0.0042650709688818768\n",
      "Iter: 108 loss: 0.0047111663415132224\n",
      "Iter: 109 loss: 0.0040503992459920935\n",
      "Iter: 110 loss: 0.0039739575034772627\n",
      "Iter: 111 loss: 0.0039614041475801486\n",
      "Iter: 112 loss: 0.0039134418917895937\n",
      "Iter: 113 loss: 0.0039134420343786176\n",
      "Iter: 114 loss: 0.0038635457081554426\n",
      "Iter: 115 loss: 0.0038078380774127485\n",
      "Iter: 116 loss: 0.003798709751048325\n",
      "Iter: 117 loss: 0.0036605895465757175\n",
      "Iter: 118 loss: 0.0035983293558185833\n",
      "Iter: 119 loss: 0.0035316452549938944\n",
      "Iter: 120 loss: 0.0031562086571573166\n",
      "Iter: 121 loss: 0.0031561499395172756\n",
      "Iter: 122 loss: 0.003071252435244374\n",
      "Iter: 123 loss: 0.00302452065137624\n",
      "Iter: 124 loss: 0.0029398640356448217\n",
      "Iter: 125 loss: 0.0035964876044689616\n",
      "Iter: 126 loss: 0.0029327459185879835\n",
      "Iter: 127 loss: 0.0028857468596045141\n",
      "Iter: 128 loss: 0.0037229491725159448\n",
      "Iter: 129 loss: 0.0028856201926740969\n",
      "Iter: 130 loss: 0.0028702448037473062\n",
      "Iter: 131 loss: 0.0028678884630240859\n",
      "Iter: 132 loss: 0.0028374732365756689\n",
      "Iter: 133 loss: 0.0029585449972304497\n",
      "Iter: 134 loss: 0.0028320671346705822\n",
      "Iter: 135 loss: 0.002789275417859997\n",
      "Iter: 136 loss: 0.0028827389706290851\n",
      "Iter: 137 loss: 0.00277213267496431\n",
      "Iter: 138 loss: 0.0027369721573332292\n",
      "Iter: 139 loss: 0.002919801600051725\n",
      "Iter: 140 loss: 0.0027295008054528075\n",
      "Iter: 141 loss: 0.0026786805139471506\n",
      "Iter: 142 loss: 0.0026741170821663125\n",
      "Iter: 143 loss: 0.0025978636394791551\n",
      "Iter: 144 loss: 0.0028428841612673944\n",
      "Iter: 145 loss: 0.0025691891578303143\n",
      "Iter: 146 loss: 0.0025517283180718476\n",
      "Iter: 147 loss: 0.0027283992994298106\n",
      "Iter: 148 loss: 0.0025510588108694316\n",
      "Iter: 149 loss: 0.0025395430555366226\n",
      "Iter: 150 loss: 0.0025270769271264789\n",
      "Iter: 151 loss: 0.0025249722724134541\n",
      "Iter: 152 loss: 0.0024897138731159439\n",
      "Iter: 153 loss: 0.0027449027728788608\n",
      "Iter: 154 loss: 0.0024847902553423833\n",
      "Iter: 155 loss: 0.0024696120304834005\n",
      "Iter: 156 loss: 0.00246928900206151\n",
      "Iter: 157 loss: 0.0024586836422907628\n",
      "Iter: 158 loss: 0.0025621815146225271\n",
      "Iter: 159 loss: 0.002458274357302503\n",
      "Iter: 160 loss: 0.0024495957113175636\n",
      "Iter: 161 loss: 0.0024265775241225931\n",
      "Iter: 162 loss: 0.0026330666521806245\n",
      "Iter: 163 loss: 0.0024220687790094\n",
      "Iter: 164 loss: 0.0022928514626281348\n",
      "Iter: 165 loss: 0.0060755037526070595\n",
      "Iter: 166 loss: 0.00228985244453482\n",
      "Iter: 167 loss: 0.0022374890311620235\n",
      "Iter: 168 loss: 0.0021691253820282216\n",
      "Iter: 169 loss: 0.0021192755878181755\n",
      "Iter: 170 loss: 0.0021122564809891008\n",
      "Iter: 171 loss: 0.0020930537810035484\n",
      "Iter: 172 loss: 0.0020929601600528746\n",
      "Iter: 173 loss: 0.0022112359467275108\n",
      "Iter: 174 loss: 0.002090371379319932\n",
      "Iter: 175 loss: 0.0020855961707883651\n",
      "Iter: 176 loss: 0.0021145500795041196\n",
      "Iter: 177 loss: 0.0020849934152904637\n",
      "Iter: 178 loss: 0.0020738582547639968\n",
      "Iter: 179 loss: 0.002042407795160045\n",
      "Iter: 180 loss: 0.0024072945751742961\n",
      "Iter: 181 loss: 0.0020359038459748678\n",
      "Iter: 182 loss: 0.0019561366041018951\n",
      "Iter: 183 loss: 0.00233151393391409\n",
      "Iter: 184 loss: 0.00192422247515252\n",
      "Iter: 185 loss: 0.0018327274135261046\n",
      "Iter: 186 loss: 0.0018267817380203891\n",
      "Iter: 187 loss: 0.0019306449588084038\n",
      "Iter: 188 loss: 0.0017982437244087528\n",
      "Iter: 189 loss: 0.0017628134186656484\n",
      "Iter: 190 loss: 0.0017627126020294285\n",
      "Iter: 191 loss: 0.0017075018900155378\n",
      "Iter: 192 loss: 0.002139640437292692\n",
      "Iter: 193 loss: 0.0017002900789858497\n",
      "Iter: 194 loss: 0.0025507639059447148\n",
      "Iter: 195 loss: 0.0016824848989546561\n",
      "Iter: 196 loss: 0.0016221684829770531\n",
      "Iter: 197 loss: 0.0016182353410636469\n",
      "Iter: 198 loss: 0.001554165858308709\n",
      "Iter: 199 loss: 0.0039432566605419692\n",
      "Iter: 200 loss: 0.0015530423139016529\n",
      "Iter: 201 loss: 0.0014743721501902893\n",
      "Iter: 202 loss: 0.0014740742959620636\n",
      "Iter: 203 loss: 0.0015563172714217807\n",
      "Iter: 204 loss: 0.0014354288308638594\n",
      "Iter: 205 loss: 0.0013820233682600778\n",
      "Iter: 206 loss: 0.002101147716091465\n",
      "Iter: 207 loss: 0.0013810667333835683\n",
      "Iter: 208 loss: 0.0013643005107122175\n",
      "Iter: 209 loss: 0.001364123112652811\n",
      "Iter: 210 loss: 0.0013526081554620286\n",
      "Iter: 211 loss: 0.0014272410887669661\n",
      "Iter: 212 loss: 0.0013513737488042162\n",
      "Iter: 213 loss: 0.001345914181264087\n",
      "Iter: 214 loss: 0.0014158180935590363\n",
      "Iter: 215 loss: 0.0013456442477794803\n",
      "Iter: 216 loss: 0.001319237388607766\n",
      "Iter: 217 loss: 0.0014633326820235404\n",
      "Iter: 218 loss: 0.0013153395892291594\n",
      "Iter: 219 loss: 0.0012904026109692099\n",
      "Iter: 220 loss: 0.001335865898044905\n",
      "Iter: 221 loss: 0.0012779661674506718\n",
      "Iter: 222 loss: 0.0016000796858098795\n",
      "Iter: 223 loss: 0.0012615411258966442\n",
      "Iter: 224 loss: 0.0012346140537870494\n",
      "Iter: 225 loss: 0.0012690195301249957\n",
      "Iter: 226 loss: 0.0012241671670165434\n",
      "Iter: 227 loss: 0.0011906374271904063\n",
      "Iter: 228 loss: 0.0014835651241292644\n",
      "Iter: 229 loss: 0.0011879956346144\n",
      "Iter: 230 loss: 0.0011823783475681635\n",
      "Iter: 231 loss: 0.0012030112255337238\n",
      "Iter: 232 loss: 0.0011810202520887087\n",
      "Iter: 233 loss: 0.0011730883236838953\n",
      "Iter: 234 loss: 0.0011645961023250885\n",
      "Iter: 235 loss: 0.0011633200881216573\n",
      "Iter: 236 loss: 0.0011477963089795125\n",
      "Iter: 237 loss: 0.0011922715581965055\n",
      "Iter: 238 loss: 0.0011417088935507663\n",
      "Iter: 239 loss: 0.0011307983141375863\n",
      "Iter: 240 loss: 0.0012678140382910805\n",
      "Iter: 241 loss: 0.0011307064368886316\n",
      "Iter: 242 loss: 0.0011564162135425436\n",
      "Iter: 243 loss: 0.0011281877463392531\n",
      "Iter: 244 loss: 0.0011260922865508468\n",
      "Iter: 245 loss: 0.0011342843662308637\n",
      "Iter: 246 loss: 0.0011256060099329151\n",
      "Iter: 247 loss: 0.0011236351938765316\n",
      "Iter: 248 loss: 0.0011190937146027195\n",
      "Iter: 249 loss: 0.001168112057501385\n",
      "Iter: 250 loss: 0.0011187361811161526\n",
      "Iter: 251 loss: 0.0011064980967978246\n",
      "Iter: 252 loss: 0.0011176546383234983\n",
      "Iter: 253 loss: 0.0010981123743995726\n",
      "Iter: 254 loss: 0.0011058783218607232\n",
      "Iter: 255 loss: 0.0010917535443995586\n",
      "Iter: 256 loss: 0.0010807213892175221\n",
      "Iter: 257 loss: 0.0010676214303371667\n",
      "Iter: 258 loss: 0.0010663855121441474\n",
      "Iter: 259 loss: 0.0010424943410565085\n",
      "Iter: 260 loss: 0.0010385325832505733\n",
      "Iter: 261 loss: 0.0010241798028731118\n",
      "Iter: 262 loss: 0.00118963579536683\n",
      "Iter: 263 loss: 0.0010078437789260816\n",
      "Iter: 264 loss: 0.0025180937040980571\n",
      "Iter: 265 loss: 0.0010003552978695557\n",
      "Iter: 266 loss: 0.001259827212952558\n",
      "Iter: 267 loss: 0.00098914406485146617\n",
      "Iter: 268 loss: 0.0010439696095961479\n",
      "Iter: 269 loss: 0.00097233660101340188\n",
      "Iter: 270 loss: 0.00094622561247371863\n",
      "Iter: 271 loss: 0.0017861818801323207\n",
      "Iter: 272 loss: 0.000946223348082994\n",
      "Iter: 273 loss: 0.00092401910033462255\n",
      "Iter: 274 loss: 0.0010209561235299531\n",
      "Iter: 275 loss: 0.00091973434652266144\n",
      "Iter: 276 loss: 0.00090466650925837206\n",
      "Iter: 277 loss: 0.00090394229201201453\n",
      "Iter: 278 loss: 0.00089487033417166547\n",
      "Iter: 279 loss: 0.00089239535131138584\n",
      "Iter: 280 loss: 0.00088136941759482429\n",
      "Iter: 281 loss: 0.00089167723459516481\n",
      "Iter: 282 loss: 0.00087493174324035339\n",
      "Iter: 283 loss: 0.00084568379038621271\n",
      "Iter: 284 loss: 0.00096955655084902123\n",
      "Iter: 285 loss: 0.00083763927130167258\n",
      "Iter: 286 loss: 0.00078064304220082088\n",
      "Iter: 287 loss: 0.0007791400892387247\n",
      "Iter: 288 loss: 0.00075377760327367284\n",
      "Iter: 289 loss: 0.00083771514848531015\n",
      "Iter: 290 loss: 0.00074676735651759286\n",
      "Iter: 291 loss: 0.00074377335863359622\n",
      "Iter: 292 loss: 0.00074321635249347375\n",
      "Iter: 293 loss: 0.00074059813585870132\n",
      "Iter: 294 loss: 0.00074310366558784271\n",
      "Iter: 295 loss: 0.000739068942817313\n",
      "Iter: 296 loss: 0.00073320844268372817\n",
      "Iter: 297 loss: 0.00074294770809684948\n",
      "Iter: 298 loss: 0.00073011463295232686\n",
      "Iter: 299 loss: 0.00072187824983627343\n",
      "Iter: 300 loss: 0.00076893917129495538\n",
      "Iter: 301 loss: 0.00072075556171928936\n",
      "Iter: 302 loss: 0.00071653916962750538\n",
      "Iter: 303 loss: 0.00071534026083454273\n",
      "Iter: 304 loss: 0.00071133154176926622\n",
      "Iter: 305 loss: 0.00071984639415241362\n",
      "Iter: 306 loss: 0.00070974508813083565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12684/1052981164.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mX_u_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_stitch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlbfgs_minimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_position\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\lbfgs.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(value_and_gradients_function, initial_position, previous_optimizer_results, num_correction_pairs, tolerance, x_tolerance, f_relative_tolerance, initial_inverse_hessian_estimate, max_iterations, parallel_iterations, stopping_condition, max_line_search_iterations, f_absolute_tolerance, name)\u001b[0m\n\u001b[0;32m    283\u001b[0m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_optimizer_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m     return tf.while_loop(\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_body\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m   2505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2506\u001b[0m   \"\"\"\n\u001b[1;32m-> 2507\u001b[1;33m   return while_loop(\n\u001b[0m\u001b[0;32m   2508\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2509\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2752\u001b[0m                                               list(loop_vars))\n\u001b[0;32m   2753\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2754\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\lbfgs.py\u001b[0m in \u001b[0;36m_body\u001b[1;34m(current_state)\u001b[0m\n\u001b[0;32m    257\u001b[0m       \u001b[1;31m# search direction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m       next_state = bfgs_utils.line_search_step(\n\u001b[0m\u001b[0;32m    260\u001b[0m           \u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_direction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m           \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_relative_tolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopping_condition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\bfgs_utils.py\u001b[0m in \u001b[0;36mline_search_step\u001b[1;34m(state, value_and_gradients_function, search_direction, grad_tolerance, f_relative_tolerance, x_tolerance, stopping_condition, max_iterations, f_absolute_tolerance)\u001b[0m\n\u001b[0;32m    207\u001b[0m                            full_gradient=state.objective_gradient)\n\u001b[0;32m    208\u001b[0m   \u001b[0minactive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m   ls_result = linesearch.hager_zhang(\n\u001b[0m\u001b[0;32m    210\u001b[0m       \u001b[0mline_search_value_grad_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m       \u001b[0minitial_step_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\hager_zhang.py\u001b[0m in \u001b[0;36mhager_zhang\u001b[1;34m(value_and_gradients_function, initial_step_size, value_at_initial_step, value_at_zero, converged, threshold_use_approximate_wolfe_condition, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param, max_iterations, name)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0minit_active\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0minit_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0minit_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     return prefer_static.cond(\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_active\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0m_apply_bracket_and_search\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\prefer_static.py\u001b[0m in \u001b[0;36mcond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m    258\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\hager_zhang.py\u001b[0m in \u001b[0;36m_apply_bracket_and_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply_bracket_and_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m       \u001b[1;34m\"\"\"Bracketing and searching to do for valid inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m       return _bracket_and_search(\n\u001b[0m\u001b[0;32m    269\u001b[0m           \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_lim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m           \u001b[0mshrinkage_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpansion_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msufficient_decrease_param\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\hager_zhang.py\u001b[0m in \u001b[0;36m_bracket_and_search\u001b[1;34m(value_and_gradients_function, init_interval, f_lim, max_iterations, shrinkage_param, expansion_param, sufficient_decrease_param, curvature_param)\u001b[0m\n\u001b[0;32m    369\u001b[0m       right=bracket_result.right)\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m   return _line_search_after_bracketing(\n\u001b[0m\u001b[0;32m    372\u001b[0m       \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_search_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[0mf_lim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msufficient_decrease_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurvature_param\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\hager_zhang.py\u001b[0m in \u001b[0;36m_line_search_after_bracketing\u001b[1;34m(value_and_gradients_function, search_interval, val_0, f_lim, max_iterations, sufficient_decrease_param, curvature_param, shrinkage_param)\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverged\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverged\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0minterval_shrunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m   return tf.while_loop(\n\u001b[0m\u001b[0;32m    499\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_loop_cond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_loop_body\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m   2505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2506\u001b[0m   \"\"\"\n\u001b[1;32m-> 2507\u001b[1;33m   return while_loop(\n\u001b[0m\u001b[0;32m   2508\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2509\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2752\u001b[0m                                               list(loop_vars))\n\u001b[0;32m   2753\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2754\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\hager_zhang.py\u001b[0m in \u001b[0;36m_loop_body\u001b[1;34m(curr_interval)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[0mactive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverged\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mcurr_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# TODO(b/208441613): Skip updates for batch members that are not active?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     secant2_raw_result = hzl.secant2(\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_lim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         sufficient_decrease_param, curvature_param)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\linesearch\\internal\\hager_zhang_lib.py\u001b[0m in \u001b[0;36msecant2\u001b[1;34m(value_and_gradients_function, val_0, search_interval, f_lim, sufficient_decrease_param, curvature_param, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'secant2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m# This will always be s.t. left <= c <= right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     val_c = value_and_gradients_function(\n\u001b[0m\u001b[0;32m    140\u001b[0m         _secant(search_interval.left, search_interval.right))\n\u001b[0;32m    141\u001b[0m     \u001b[0mfinished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_negative_inf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\optimizer\\bfgs_utils.py\u001b[0m in \u001b[0;36m_restricted_func\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m     \u001b[0mobjective_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     return ValueAndGradient(\n\u001b[0;32m    312\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "func = function_factory(model, tf.keras.losses.MeanSquaredError() ,X_u_train , u_train)\n",
    "init_params = tf.dynamic_stitch(func.idx, model.trainable_variables)\n",
    "results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=func, initial_position=init_params, max_iterations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "func.assign_new_model_parameters(results.position)\n",
    "p=np.vstack([X_f_train,X_u_train])\n",
    "q=np.vstack([model.predict(X_f_train),model.predict(X_u_train)])[:,0]\n",
    "plot_helper(p, q, \"u (x,y)\", \"pred_soln3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splicenum = 0\n",
    "plt.plot(data[splicenum,:], label = \"Y =\"+str(y[splicenum]))\n",
    "plt.plot(data[:,splicenum], label = \"X = \"+str(x[splicenum]))\n",
    "\n",
    "splice1  = np.array([x[splicenum]*np.ones(len(data[splicenum, :])),y ]).T\n",
    "splice2 = np.array([x, y[splicenum]*np.ones(len(data[:, splicenum])) ]).T\n",
    "\n",
    "plt.plot(model.predict(splice1), label = \"X Approx\")\n",
    "plt.plot(model.predict(splice2), label = \"Y Approx\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070be201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_f_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    " len(X_f_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5438a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_helper(X_star, data, \"exact\", \"exact.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22e69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4e97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9f4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
